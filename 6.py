# -*- coding: utf-8 -*-
"""k-means-clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsHVXbU3vO_TEDCWJOJYB4o0wA7r6lvC

#### Implement K-Means clustering/ hierarchical clustering on sales_data_sample.csv dataset. Determine the number of clusters using the elbow method.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns

df = pd.read_csv("https://raw.githubusercontent.com/sahil-gidwani/ML/main/dataset/sales_data_sample.csv", encoding="latin")
df

df.dtypes

X = df.iloc[:, [3,4]].values

df.describe()

# mean is far from std this indicates high variance
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X = ss.fit_transform(X)

wcss = []  # Within-Cluster-Sum-of-Squares

# WCSS(K) = Σ d(Ci, Mi)^2
# Where:
# - WCSS(K) is the Within-Cluster-Sum-of-Squares for a specific value of K (number of clusters).
# - Σ represents the sum over all clusters (i = 1 to K), where K is the number of clusters.
# - Ci represents a data point in cluster i.
# - Mi represents the centroid (mean) of cluster i.
# - d(Ci, Mi) is the Euclidean distance between data point Ci and the centroid Mi of its cluster.
# - The goal in K-Means clustering is to minimize the WCSS by finding the optimal number of clusters (K). Typically, the "elbow point" in the plot of WCSS against K is chosen as the optimal number of clusters because it indicates the point where increasing the number of clusters doesn't significantly reduce the WCSS.
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init="k-means++", random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the WCSS values
ks = range(1, 11)
plt.plot(ks, wcss, 'bx-')
plt.title("Elbow Method")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("WCSS")
plt.show()

optimal_k = 3

kmeans_optimal = KMeans(n_clusters=optimal_k, init="k-means++", random_state=42)

# Get the cluster labels for each data point
cluster_labels = kmeans_optimal.fit_predict(X)

# Visualize the clusters
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')
plt.scatter(kmeans_optimal.cluster_centers_[:, 0], kmeans_optimal.cluster_centers_[:, 1], s=300, c='red', label='Centroids')
plt.title(f"K-Means Clustering (Number of Clusters: {3})")
plt.legend()
plt.show()

from sklearn.cluster import AgglomerativeClustering

# affinity='euclidean': The 'affinity' parameter determines the distance metric used to measure the dissimilarity between data points. 'euclidean' is a common choice, and it calculates the Euclidean distance between data points in a multi-dimensional space. It's suitable for cases where the data features are continuous and numerical, which is often the case in clustering problems.
# linkage='ward': The 'linkage' parameter specifies the linkage criterion to determine how the distance between clusters is measured. 'ward' is one of the linkage methods. It uses the Ward variance minimization algorithm to measure the distance between clusters, aiming to minimize the increase in variance when two clusters are merged. This method is suitable for hierarchical clustering.
hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)

# Create a DataFrame to hold the data and cluster labels
data = pd.DataFrame({'X-axis': X[:, 0], 'Y-axis': X[:, 1], 'Cluster': y_hc})

# Create a scatterplot with different colors for each cluster
sns.scatterplot(data=data, x='X-axis', y='Y-axis', hue='Cluster', palette='viridis')
plt.title('Hierarchical Clustering')
plt.show()