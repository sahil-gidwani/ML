# -*- coding: utf-8 -*-
"""Diabetes classification using KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1smL9jRwVk_C7I-lgV0_004NRinYaXuT2

# Diabetes classification using KNN
"""

import pandas as pd
import numpy as np

data = pd.read_csv("https://raw.githubusercontent.com/sahil-gidwani/ML/main/dataset/diabetes.csv")
data.head()

data.isnull().any()

data.describe().T

# Glucose, BloodPressure, SkinThickness, Insulin, BMI columns have values 0 which does not make sense, hence are missing values
data_copy = data.copy(deep = True)
data_copy[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data_copy[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)
data_copy.isnull().sum()

# To fill these Nan values the data distribution needs to be understood
p = data.hist(figsize = (20, 20))

data_copy['Glucose'].fillna(data_copy['Glucose'].mean(), inplace = True)
data_copy['BloodPressure'].fillna(data_copy['BloodPressure'].mean(), inplace = True)
data_copy['SkinThickness'].fillna(data_copy['SkinThickness'].median(), inplace = True)
data_copy['Insulin'].fillna(data_copy['Insulin'].median(), inplace = True)
data_copy['BMI'].fillna(data_copy['BMI'].median(), inplace = True)

p = data_copy.hist(figsize = (20, 20))

p = data.Outcome.value_counts().plot(kind = "bar")

# The above graph shows that the data is biased towards datapoints having outcome value as 0 where it means that diabetes was not present actually. The number of non-diabetics is almost twice the number of diabetic patients
import seaborn as sns
p = sns.pairplot(data_copy, hue = 'Outcome')

import matplotlib.pyplot as plt
plt.figure(figsize = (12, 10))
p = sns.heatmap(data.corr(), annot = True, cmap ='RdYlGn')

plt.figure(figsize = (12, 10))
p = sns.heatmap(data_copy.corr(), annot = True, cmap ='RdYlGn')

# StandardScaler is a data preprocessing technique commonly used in machine learning and statistics to scale or standardize the features (variables) of a dataset. It transforms the data in such a way that the scaled features have a mean of 0 and a standard deviation of 1. This process is also known as z-score normalization or standardization.
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X = pd.DataFrame(sc_X.fit_transform(data_copy.drop(["Outcome"], axis = 1)), columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age'])

X.head()

Y = data_copy.Outcome

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 1/3, random_state = 42, stratify = Y)

from sklearn.neighbors import KNeighborsClassifier

train_scores = []
test_scores = []
best_k = None
best_test_score = 0.0

# Choose an odd number to avoid tie situations
for k in range(1, 30, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, Y_train)
    # knn.score() is a meausre of accuracy = TP + TN / TP + TN + FP + FN
    train_score = knn.score(X_train, Y_train)
    test_score = knn.score(X_test, Y_test)
    train_scores.append(train_score)
    test_scores.append(test_score)

    # Check if the current k results in a higher accuracy
    if test_score > best_test_score:
        best_k = k
        best_test_score = test_score

print(f"Best k: {best_k}")

plt.figure(figsize = (12, 5))
plt.title('Accuracy vs k')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy Score')
p = sns.lineplot(x = range(1, 30, 2), y = train_scores, marker = '*', label = 'Train Score', markers = True)
p = sns.lineplot(x = range(1, 30, 2), y = test_scores, marker = 'o', label = 'Test Score', markers = True)

# Setup a knn classifier with best_k neighbors
knn = KNeighborsClassifier(n_neighbors=best_k)

knn.fit(X_train, Y_train)
knn.score(X_test, Y_test)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score

Y_pred = knn.predict(X_test)
cnf_matrix = confusion_matrix(Y_test, Y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=knn.classes_)
disp.plot()

def model_evaluation(y_test, y_pred, model_name):
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    f2 = fbeta_score(y_test, y_pred, beta = 2.0)

    results = pd.DataFrame([[model_name, acc, prec, rec, f1, f2]],
                       columns = ["Model", "Accuracy", "Precision", "Recall",
                                 "F1 SCore", "F2 Score"])
    results = results.sort_values(["Precision", "Recall", "F2 Score"], ascending = False)
    return results

model_evaluation(Y_test, Y_pred, "KNN")

from sklearn.metrics import auc, roc_auc_score, roc_curve

# predict_proba() is used to predict the class probabilities
# [:,-1]: This slice notation selects the last column of the probability matrix, which corresponds to the probability of the positive class
Y_pred_proba = knn.predict_proba(X_test)[:,-1]
fpr, tpr, threshold = roc_curve(Y_test, Y_pred_proba)

classifier_roc_auc = roc_auc_score(Y_test, Y_pred_proba)
plt.plot([0,1], [0,1], label = "(area = 0.5)")

plt.plot(fpr, tpr, label ='KNN (area = %0.2f)' % classifier_roc_auc)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title(f'Knn(n_neighbors = {best_k}) ROC curve')
plt.legend(loc = "lower right", fontsize = "medium")
plt.show()