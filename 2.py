# -*- coding: utf-8 -*-
"""classify-the-email-using-the-binary-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PXm81QNAdzT5EARpKZT6ZN0OPj1CFe_e

Classify the email using the binary classification method. Email Spam detection has two states:

1.   Normal State – Not Spam
2.   Abnormal State – Spam

Use K-Nearest Neighbors and Support Vector Machine for classification. Analyze their performance.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import matplotlib.pyplot as plt

df = pd.read_csv('https://raw.githubusercontent.com/sahil-gidwani/ML/main/dataset/emails.csv')
df

df.shape

df.isnull().any()

df.drop(columns = 'Email No.', inplace = True)
df

df.columns

df.Prediction.unique()

df['Prediction'] = df['Prediction'].replace({0 : 'Not spam', 1 : 'Spam'})
df

X = df.drop(columns = 'Prediction', axis = 1)
Y = df['Prediction']

X.columns

Y.head()

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)

# KNN
k_values = []
accuracy_scores = []

best_k = None
best_accuracy = 0.0

# Choose an odd number to avoid tie situations
for k in range(1, 21, 2):  # You can adjust the range based on your preferences
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(x_train, y_train)
    y_pred = knn.predict(x_test)
    accuracy = metrics.accuracy_score(y_test, y_pred)
    k_values.append(k)
    accuracy_scores.append(accuracy)

    # Check if the current k results in a higher accuracy
    if accuracy > best_accuracy:
        best_k = k
        best_accuracy = accuracy

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracy_scores, marker='o', linestyle='-', color='b')
plt.title('Accuracy vs k')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy Score')
plt.grid(True)
plt.show()

knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_test)

print(f"Best k: {best_k}")

print("Prediction:")
print(y_pred)

M = metrics.accuracy_score(y_test, y_pred)
print("KNN accuracy: ", M)

C = metrics.confusion_matrix(y_test, y_pred)
disp = metrics.ConfusionMatrixDisplay(confusion_matrix=C, display_labels=knn.classes_)
disp.plot()

# SVM Classifier
model = SVC()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)

print("Prediction:")
print(y_pred)

kc = metrics.accuracy_score(y_test, y_pred)
print("SVM accuracy: ", kc)

C = metrics.confusion_matrix(y_test, y_pred)
disp = metrics.ConfusionMatrixDisplay(confusion_matrix=C, display_labels=model.classes_)
disp.plot()

# [[TN  FP]
#  [FN  TP]]

# # Example confusion matrix
# C = metrics.confusion_matrix(y_test, y_pred)

# # Extract TP, TN, FP, FN
# TN = C[0, 0]  # True Negatives
# FP = C[0, 1]  # False Positives
# FN = C[1, 0]  # False Negatives
# TP = C[1, 1]  # True Positives

# # Calculate accuracy
# accuracy = (TP + TN) / (TP + TN + FP + FN)

# # Calculate precision
# precision = TP / (TP + FP)

# # Calculate recall
# recall = TP / (TP + FN)

# # Calculate specificity (True Negative Rate)
# specificity = TN / (TN + FP)

# # Calculate F1-Score
# f1_score = 2 * (precision * recall) / (precision + recall)

# # Calculate False Positive Rate
# fpr = FP / (FP + TN)

# # Calculate False Negative Rate
# fnr = FN / (FN + TP)

# # Print the calculated metrics
# print("Accuracy:", accuracy)
# print("Precision:", precision)
# print("Recall:", recall)
# print("Specificity:", specificity)
# print("F1-Score:", f1_score)
# print("False Positive Rate:", fpr)
# print("False Negative Rate:", fnr)